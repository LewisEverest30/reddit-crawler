# coding=utf-8
import json
import logging
import random
import time
import asyncio
import os
from pathlib import Path
from playwright.async_api import async_playwright
import datetime
import traceback
from urllib.parse import urlparse, parse_qs

def setup_logger():
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler("reddit_crawler_playwright.log", encoding="utf-8"),
            logging.StreamHandler()
        ]
    )

class RedditCrawler:
    def __init__(self, subreddit_url, output_file="reddit_data_with_time.json", 
                 max_posts=1000, headless=False, max_failures=3, 
                 delays=None, viewport=None, user_agent=None, 
                 user_data_dir=None, resume_from_post=1, sampling_ratios=None):
        """
        åˆå§‹åŒ–Redditçˆ¬è™«
        
        Args:
            subreddit_url: Redditå­ç‰ˆå—URLæˆ–å…·ä½“å¸–å­URL
            output_file: è¾“å‡ºJSONæ–‡ä»¶å
            max_posts: æœ€å¤§çˆ¬å–å¸–å­æ•°é‡
            headless: æ˜¯å¦æ— å¤´æ¨¡å¼
            max_failures: æœ€å¤§è¿ç»­å¤±è´¥æ¬¡æ•°
            delays: å»¶è¿Ÿé…ç½®å­—å…¸
            viewport: è§†çª—å¤§å°é…ç½®
            user_agent: ç”¨æˆ·ä»£ç†
            user_data_dir: æµè§ˆå™¨ç”¨æˆ·æ•°æ®ç›®å½•
            resume_from_post: ä»ç¬¬å‡ ä¸ªå¸–å­å¼€å§‹çˆ¬å–
            sampling_ratios: é‡‡æ ·æ¯”ä¾‹é…ç½®å­—å…¸ï¼Œä¾‹å¦‚ {'new': 0.65, 'top_year': 0.25, 'best': 0.10}
        """
        self.subreddit_url = subreddit_url
        self.output_file = output_file
        self.max_posts = max_posts
        self.headless = headless
        self.max_failures = max_failures
        self.resume_from_post = resume_from_post
        self.user_data_dir = user_data_dir or "./reddit_browser_data"
        
        # é…ç½®å»¶è¿Ÿç­–ç•¥
        self.delays = delays or {
            'page_min': 2000, 'page_max': 5000,
            'action_min': 500, 'action_max': 1500,
            'scroll_min': 1000, 'scroll_max': 3000,
            'api_min': 1000, 'api_max': 2000
        }
        
        # é…ç½®é‡‡æ ·æ¯”ä¾‹ç­–ç•¥
        self.sampling_ratios = sampling_ratios or {
            'new': 0.65,        # 65% - æœ€æ–°å¸–å­
            'top_year': 0.25,   # 25% - å¹´åº¦çƒ­é—¨
            'best': 0.10        # 10% - æœ€ä½³å¸–å­
        }
        
        # éªŒè¯é‡‡æ ·æ¯”ä¾‹æ€»å’Œ
        total_ratio = sum(self.sampling_ratios.values())
        if abs(total_ratio - 1.0) > 0.01:  # å…è®¸å°çš„æµ®ç‚¹è¯¯å·®
            logging.warning(f"é‡‡æ ·æ¯”ä¾‹æ€»å’Œä¸º {total_ratio:.3f}ï¼Œä¸ç­‰äº1.0ï¼Œå°†è‡ªåŠ¨å½’ä¸€åŒ–")
            # å½’ä¸€åŒ–æ¯”ä¾‹
            for key in self.sampling_ratios:
                self.sampling_ratios[key] /= total_ratio
        
        logging.info(f"é‡‡æ ·æ¯”ä¾‹é…ç½®: {self.sampling_ratios}")
        
        # é…ç½®æµè§ˆå™¨å‚æ•°
        self.viewport = viewport or {'width': 1920, 'height': 1080}
        self.user_agent = user_agent or 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        
        # æå–subredditåç§°å¹¶åˆ›å»ºå¯¹åº”ç›®å½•
        self.subreddit_name = self.extract_subreddit_name(subreddit_url)
        self.subreddit_dir = f"./outputs/{self.subreddit_name}"
        
        # ä¸ºè¯¥subredditåˆ›å»ºä¸“ç”¨ç›®å½•
        Path(self.subreddit_dir).mkdir(parents=True, exist_ok=True)
        
        # è¿›åº¦çŠ¶æ€æ–‡ä»¶ï¼ˆåˆå¹¶è¿›åº¦å’ŒURLåˆ—è¡¨ï¼‰
        self.state_file = os.path.join(self.subreddit_dir, "reddit_crawler_state.json")
        
        # å¦‚æœè¾“å‡ºæ–‡ä»¶æ²¡æœ‰æŒ‡å®šè·¯å¾„ï¼Œä¹Ÿä¿å­˜åœ¨subredditç›®å½•ä¸‹
        if not os.path.dirname(output_file):
            self.output_file = os.path.join(self.subreddit_dir, output_file)
        else:
            self.output_file = output_file
        
        # å­˜å‚¨çˆ¬å–çš„æ•°æ®
        self.all_posts_data = []
        self.collected_urls = set()
        
        # åˆ›å»ºç”¨æˆ·æ•°æ®ç›®å½•
        Path(self.user_data_dir).mkdir(exist_ok=True)
        
        # è®°å½•å½“å‰é…ç½®
        logging.info(f"Subreddit: {self.subreddit_name}")
        logging.info(f"æ•°æ®ç›®å½•: {self.subreddit_dir}")
        logging.info(f"è¾“å‡ºæ–‡ä»¶: {self.output_file}")
        
        # åˆå§‹åŒ–playwrightç›¸å…³å˜é‡
        self.browser = None
        self.context = None
        self.page = None

    def extract_subreddit_name(self, url):
        """ä»Reddit URLä¸­æå–subredditåç§°"""
        try:
            import re
            # åŒ¹é… /r/subreddit_name/ æ ¼å¼
            match = re.search(r'/r/([^/]+)', url)
            if match:
                return match.group(1)
            # å¦‚æœæ˜¯å¸–å­è¯¦æƒ…é¡µï¼Œä¹Ÿå°è¯•æå–
            if '/comments/' in url:
                parts = url.split('/r/')
                if len(parts) > 1:
                    subreddit_part = parts[1].split('/')[0]
                    return subreddit_part
            return "unknown_subreddit"
        except Exception as e:
            logging.warning(f"æå–subredditåç§°å¤±è´¥: {e}")
            return "unknown_subreddit"
    
    def extract_post_id(self, url):
        """ä»Reddit URLä¸­æå–å¸–å­IDç”¨äºå»é‡"""
        try:
            import re
            match = re.search(r'/comments/([a-zA-Z0-9]+)/', url)
            return match.group(1) if match else None
        except Exception as e:
            logging.debug(f"æå–å¸–å­IDå¤±è´¥: {e}")
            return None

    def convert_time(self, timestamp):
        """å°†UTCæ—¶é—´æˆ³è½¬æ¢ä¸ºå¯è¯»æ—¶é—´æ ¼å¼"""
        if not timestamp:
            return "N/A"
        try:
            return datetime.datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')
        except:
            return "N/A"

    def parse_json_comment(self, comment_data):
        """
        é€’å½’è§£æReddit APIè¿”å›çš„è¯„è®ºJSONæ•°æ®
        
        Args:
            comment_data: Reddit APIè¿”å›çš„è¯„è®ºæ•°æ®å­—å…¸
            
        Returns:
            dict: è§£æåçš„è¯„è®ºæ•°æ®
        """
        if comment_data.get('kind') == 'more':
            return None

        data = comment_data.get('data', {})
        
        # è·å–æ—¶é—´æˆ³
        utc_timestamp = data.get("created_utc", 0)
        
        # æå–æ ¸å¿ƒå†…å®¹
        parsed = {
            "author": data.get("author", "[Deleted]"),
            "text": data.get("body", "[æ— æ–‡æœ¬]"),
            "votes": data.get("score", 0),
            "created_utc": utc_timestamp,
            "created_time": self.convert_time(utc_timestamp),
            "replies": [],
            "reply_count": 0
        }

        # é€’å½’å¤„ç†å›å¤
        replies_raw = data.get("replies")
        
        if isinstance(replies_raw, dict):
            children = replies_raw.get('data', {}).get('children', [])
            for child in children:
                child_parsed = self.parse_json_comment(child)
                if child_parsed:
                    parsed["replies"].append(child_parsed)

        # è®¡ç®—ç›´æ¥å­å›å¤çš„æ•°é‡
        parsed["reply_count"] = len(parsed["replies"])

        return parsed

    async def init_browser(self):
        """åˆå§‹åŒ–æµè§ˆå™¨å’Œé¡µé¢"""
        try:
            logging.info("æ­£åœ¨åˆå§‹åŒ–æµè§ˆå™¨...")
            self.playwright = await async_playwright().start()
            
            # å¯åŠ¨æµè§ˆå™¨ï¼Œä½¿ç”¨ç”¨æˆ·æ•°æ®ç›®å½•æŒä¹…åŒ–çŠ¶æ€
            self.browser = await self.playwright.chromium.launch_persistent_context(
                user_data_dir=self.user_data_dir,
                headless=self.headless,
                user_agent=self.user_agent,
                viewport=self.viewport,
                locale='en-US',
                timezone_id='America/New_York',
                args=[
                    '--no-sandbox',
                    '--disable-blink-features=AutomationControlled',
                    '--disable-web-security',
                    '--disable-features=VizDisplayCompositor',
                    '--disable-dev-shm-usage',
                    '--no-first-run',
                    '--disable-notifications'
                ]
            )
            
            self.context = self.browser
            
            # éšè—webdriverç‰¹å¾
            await self.context.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined,
                });
                
                // åˆ é™¤webdriveræ ‡è¯†
                delete navigator.__proto__.webdriver;
                
                // ä¿®æ”¹æ’ä»¶ä¿¡æ¯
                Object.defineProperty(navigator, 'plugins', {
                    get: () => [1, 2, 3, 4, 5],
                });
                
                // ä¿®æ”¹è¯­è¨€ä¿¡æ¯
                Object.defineProperty(navigator, 'languages', {
                    get: () => ['en-US', 'en'],
                });
            """)
            
            # åˆ›å»ºæ–°é¡µé¢
            self.page = await self.context.new_page()
            
            # è®¾ç½®é¢å¤–çš„è¯·æ±‚å¤´
            await self.page.set_extra_http_headers({
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept-Encoding': 'gzip, deflate, br',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1'
            })
            
            logging.info("âœ… æµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logging.error(f"æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    async def simulate_human_behavior(self):
        """æ¨¡æ‹Ÿäººç±»æµè§ˆè¡Œä¸º"""
        try:
            # éšæœºé¼ æ ‡ç§»åŠ¨
            await self.page.mouse.move(
                random.randint(100, self.viewport['width']-100), 
                random.randint(100, self.viewport['height']-100)
            )
            await self.page.wait_for_timeout(random.randint(100, 500))
            
            # éšæœºæ»šåŠ¨
            scroll_distance = random.randint(200, 800)
            await self.page.mouse.wheel(0, scroll_distance)
            await self.page.wait_for_timeout(random.randint(500, 1000))
            
            # æ¨¡æ‹Ÿé˜…è¯»æ—¶é—´
            reading_time = random.randint(1000, 3000)
            await self.page.wait_for_timeout(reading_time)
            
        except Exception as e:
            logging.warning(f"æ¨¡æ‹Ÿäººç±»è¡Œä¸ºæ—¶å‡ºç°å¼‚å¸¸: {e}")

    def save_progress(self, current_post_index, collected_urls_with_source):
        """ä¿å­˜å½“å‰çˆ¬è™«çŠ¶æ€ï¼ˆè¿›åº¦+URLåˆ—è¡¨+æ¥æºä¿¡æ¯ï¼‰"""
        try:
            state_data = {
                "current_post_index": current_post_index,
                "collected_urls_with_source": collected_urls_with_source,  # [{"url": url, "source": source}]
                "total_collected": len(collected_urls_with_source),
                "subreddit_name": self.subreddit_name,
                "max_posts": self.max_posts,
                "sampling_ratios": self.sampling_ratios,  # ä¿å­˜é‡‡æ ·é…ç½®
                "last_updated": datetime.datetime.now().isoformat(),
                "version": "1.0"
            }
            
            with open(self.state_file, 'w', encoding='utf-8') as f:
                json.dump(state_data, f, ensure_ascii=False, indent=2)
                
            logging.info(f"å·²ä¿å­˜çˆ¬è™«çŠ¶æ€: ç¬¬ {current_post_index}/{len(collected_urls_with_source)} ä¸ªå¸–å­")
            
        except Exception as e:
            logging.warning(f"ä¿å­˜çˆ¬è™«çŠ¶æ€å¤±è´¥: {e}")

    def load_progress(self):
        """åŠ è½½çˆ¬è™«çŠ¶æ€ï¼ˆè¿›åº¦+URLåˆ—è¡¨+æ¥æºä¿¡æ¯ï¼‰"""
        try:
            if os.path.exists(self.state_file):
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    state_data = json.load(f)
                
                current_index = state_data.get('current_post_index', self.resume_from_post)
                last_updated = state_data.get('last_updated', 'Unknown')
                
                # è·å–URLåˆ—è¡¨
                collected_urls_with_source = state_data.get('collected_urls_with_source', [])
                
                # éªŒè¯çŠ¶æ€æ•°æ®çš„å®Œæ•´æ€§
                if len(collected_urls_with_source) == 0:
                    logging.warning("çŠ¶æ€æ–‡ä»¶ä¸­URLåˆ—è¡¨ä¸ºç©ºï¼Œå°†é‡æ–°æ”¶é›†")
                    return self.resume_from_post, []
                
                # éªŒè¯å½“å‰indexæ˜¯å¦æœ‰æ•ˆ
                if current_index > len(collected_urls_with_source):
                    logging.warning(f"è¿›åº¦ç´¢å¼•({current_index})è¶…å‡ºURLåˆ—è¡¨é•¿åº¦({len(collected_urls_with_source)})ï¼Œé‡ç½®ä¸º1")
                    current_index = 1
                
                # æ¢å¤é‡‡æ ·é…ç½®ï¼ˆå¦‚æœçŠ¶æ€æ–‡ä»¶ä¸­æœ‰ä¿å­˜ï¼‰
                if 'sampling_ratios' in state_data:
                    saved_ratios = state_data['sampling_ratios']
                    if saved_ratios != self.sampling_ratios:
                        logging.info(f"çŠ¶æ€æ–‡ä»¶ä¸­çš„é‡‡æ ·æ¯”ä¾‹: {saved_ratios}")
                        logging.info(f"å½“å‰é…ç½®çš„é‡‡æ ·æ¯”ä¾‹: {self.sampling_ratios}")
                        logging.info("ä½¿ç”¨å½“å‰é…ç½®çš„é‡‡æ ·æ¯”ä¾‹ç»§ç»­çˆ¬å–")
                
                logging.info(f"è¯»å–åˆ°çˆ¬è™«çŠ¶æ€ - è¿›åº¦: {current_index}/{len(collected_urls_with_source)}, æ›´æ–°æ—¶é—´: {last_updated}")
                return max(current_index, self.resume_from_post), collected_urls_with_source
            else:
                return self.resume_from_post, []
                
        except Exception as e:
            logging.warning(f"åŠ è½½çˆ¬è™«çŠ¶æ€å¤±è´¥: {e}")
            return self.resume_from_post, []

    def save_data(self):
        """ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶"""
        try:
            logging.info(f"æ­£åœ¨ä¿å­˜ {len(self.all_posts_data)} æ¡æ•°æ®åˆ° {self.output_file}...")
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆè¯»å–ç°æœ‰æ•°æ®
            existing_data = []
            if os.path.exists(self.output_file):
                try:
                    with open(self.output_file, 'r', encoding='utf-8') as f:
                        existing_data = json.load(f)
                    logging.info(f"è¯»å–åˆ°ç°æœ‰æ•°æ® {len(existing_data)} æ¡")
                except Exception as e:
                    logging.warning(f"è¯»å–ç°æœ‰æ•°æ®å¤±è´¥: {e}")
            
            # åˆå¹¶æ•°æ®ï¼ˆé¿å…é‡å¤ï¼‰
            all_data = existing_data + self.all_posts_data
            
            with open(self.output_file, 'w', encoding='utf-8') as f:
                json.dump(all_data, f, ensure_ascii=False, indent=4)
                
            logging.info(f"âœ… æ•°æ®ä¿å­˜æˆåŠŸï¼Œæ€»è®¡ {len(all_data)} æ¡å¸–å­")
            
        except Exception as e:
            logging.error(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")

    async def collect_post_urls(self, target_url):
        """æ”¶é›†å¸–å­URLé“¾æ¥ - æ”¯æŒéå‡åŒ€é‡‡æ ·"""
        logging.info("å¼€å§‹æ”¶é›†å¸–å­é“¾æ¥ï¼ˆéå‡åŒ€é‡‡æ ·æ¨¡å¼ï¼‰...")
        
        # åˆ¤æ–­æ˜¯è¯¦æƒ…é¡µè¿˜æ˜¯åˆ—è¡¨é¡µ
        if "/comments/" in target_url:
            logging.info("æ£€æµ‹åˆ°å½“å‰ä¸ºå¸–å­è¯¦æƒ…é¡µï¼Œåªå¤„ç†è¿™ä¸€ä¸ªå¸–å­")
            return [{"url": target_url, "source": "single_post"}]
        
        # è®¡ç®—å„ç§æ’åºæ–¹å¼éœ€è¦æ”¶é›†çš„å¸–å­æ•°é‡ï¼ˆåŸºäºé…ç½®çš„æ¯”ä¾‹ï¼‰
        sampling_counts = {}
        remaining_count = self.max_posts
        
        # æŒ‰é…ç½®æ¯”ä¾‹è®¡ç®—æ¯ç§ç±»å‹çš„æ•°é‡
        sorted_types = sorted(self.sampling_ratios.keys())  # ç¡®ä¿é¡ºåºä¸€è‡´
        for i, source_type in enumerate(sorted_types):
            if i == len(sorted_types) - 1:  # æœ€åä¸€ä¸ªç±»å‹å–å‰©ä½™çš„å…¨éƒ¨
                sampling_counts[source_type] = remaining_count
            else:
                count = int(self.max_posts * self.sampling_ratios[source_type])
                sampling_counts[source_type] = count
                remaining_count -= count
        
        # æ‰“å°é‡‡æ ·è®¡åˆ’
        plan_parts = []
        for source_type, count in sampling_counts.items():
            percentage = (count / self.max_posts) * 100
            plan_parts.append(f"{source_type}({count}, {percentage:.1f}%)")
        logging.info(f"é‡‡æ ·è®¡åˆ’: {' + '.join(plan_parts)} = {self.max_posts}")
        
        # å­˜å‚¨ç»“æœï¼š[{"url": url, "source": source_type}]
        collected_urls_with_source = []
        seen_post_ids = set()  # ç”¨äºå»é‡
        
        # æ„å»ºåŸºç¡€URLï¼ˆç§»é™¤å¯èƒ½çš„è·¯å¾„åç¼€ï¼‰
        base_subreddit_url = target_url.rstrip('/')
        if base_subreddit_url.endswith(('/hot', '/new', '/top', '/best')):
            base_subreddit_url = '/'.join(base_subreddit_url.split('/')[:-1])
        
        # é‡‡æ ·é…ç½®ï¼šæ˜ å°„source_typeåˆ°URLåç¼€
        source_url_mapping = {
            "new": "/new/",
            "top_year": "/top/?t=year",
            "best": "/best/",
            "hot": "/hot/",  # æ”¯æŒæ›´å¤šç±»å‹
            "rising": "/rising/"
        }
        
        # æ„å»ºå®é™…çš„é‡‡æ ·é…ç½®åˆ—è¡¨
        sampling_configs = []
        for source_type, count in sampling_counts.items():
            if source_type in source_url_mapping and count > 0:
                url_suffix = source_url_mapping[source_type]
                sampling_configs.append((source_type, url_suffix, count))
            else:
                logging.warning(f"æœªçŸ¥çš„é‡‡æ ·ç±»å‹æˆ–æ•°é‡ä¸º0: {source_type}({count})")
        
        for source_type, url_suffix, target_count in sampling_configs:
            if target_count <= 0:
                continue
                
            sampling_url = base_subreddit_url + url_suffix
            logging.info(f"å¼€å§‹ä» {source_type} æ”¶é›† {target_count} ä¸ªå¸–å­: {sampling_url}")
            
            try:
                collected_from_source = await self._collect_from_single_source(
                    sampling_url, source_type, target_count, seen_post_ids
                )
                collected_urls_with_source.extend(collected_from_source)
                
                logging.info(f"ä» {source_type} æˆåŠŸæ”¶é›†åˆ° {len(collected_from_source)} ä¸ªå¸–å­")
                
            except Exception as e:
                logging.error(f"ä» {source_type} æ”¶é›†å¸–å­æ—¶å‡ºé”™: {e}")
                continue
        
        logging.info(f"ğŸ¯ éå‡åŒ€é‡‡æ ·å®Œæˆï¼Œæ€»å…±æ”¶é›†åˆ° {len(collected_urls_with_source)} ä¸ªå”¯ä¸€å¸–å­")
        
        # æ‰“å°é‡‡æ ·ç»Ÿè®¡
        source_stats = {}
        for item in collected_urls_with_source:
            source = item["source"]
            source_stats[source] = source_stats.get(source, 0) + 1
        
        for source, count in source_stats.items():
            percentage = (count / len(collected_urls_with_source)) * 100 if collected_urls_with_source else 0
            logging.info(f"  {source}: {count} ä¸ªå¸–å­ ({percentage:.1f}%)")
        
        return collected_urls_with_source
    
    async def _collect_from_single_source(self, source_url, source_type, target_count, seen_post_ids):
        """ä»å•ä¸ªæ’åºé¡µé¢æ”¶é›†æŒ‡å®šæ•°é‡çš„å¸–å­URL"""
        collected_urls = []
        
        try:
            # è®¿é—®ç›®æ ‡é¡µé¢
            await self.page.goto(source_url, wait_until='domcontentloaded', timeout=30000)
            await self.page.wait_for_timeout(random.randint(2000, 4000))
            
            no_new_data_count = 0
            scroll_count = 0
            
            while len(collected_urls) < target_count and no_new_data_count < 3:
                # æ¨¡æ‹Ÿäººç±»è¡Œä¸º
                await self.simulate_human_behavior()
                
                # æå–å½“å‰å¯è§çš„æ‰€æœ‰å¸–å­é“¾æ¥
                try:
                    links = await self.page.query_selector_all('a[href*="/comments/"]')
                    new_found_count = 0
                    
                    for link in links:
                        try:
                            href = await link.get_attribute("href")
                            if href and "/user/" not in href:
                                # è½¬æ¢ç›¸å¯¹è·¯å¾„ä¸ºç»å¯¹è·¯å¾„
                                if href.startswith('/'):
                                    href = "https://www.reddit.com" + href
                                
                                # æå–å¸–å­IDè¿›è¡Œå»é‡
                                post_id = self.extract_post_id(href)
                                if post_id and post_id not in seen_post_ids:
                                    seen_post_ids.add(post_id)
                                    collected_urls.append({"url": href, "source": source_type})
                                    new_found_count += 1
                                    
                                    if len(collected_urls) >= target_count:
                                        break
                                        
                        except Exception as e:
                            logging.debug(f"å¤„ç†é“¾æ¥æ—¶å‡ºé”™: {e}")
                            continue
                    
                    current_count = len(collected_urls)
                    logging.info(f"  {source_type}: {current_count}/{target_count} (æœ¬è½®æ–°å¢: {new_found_count})")
                    
                    # æ£€æŸ¥æ˜¯å¦è·å–åˆ°æ–°æ•°æ®
                    if new_found_count == 0:
                        no_new_data_count += 1
                    else:
                        no_new_data_count = 0
                    
                    # å¦‚æœå·²ç»æ”¶é›†å¤Ÿäº†ï¼Œå°±åœæ­¢
                    if current_count >= target_count:
                        break
                    
                    # æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå†…å®¹
                    scroll_count += 1
                    await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight);")
                    
                    # ç­‰å¾…æ–°å†…å®¹åŠ è½½
                    scroll_delay = random.randint(self.delays['scroll_min'], self.delays['scroll_max'])
                    await self.page.wait_for_timeout(scroll_delay)
                        
                except Exception as e:
                    logging.error(f"{source_type}: æ”¶é›†é“¾æ¥æ—¶å‡ºé”™: {e}")
                    no_new_data_count += 1
                    await self.page.wait_for_timeout(2000)
            
            return collected_urls[:target_count]  # ç¡®ä¿ä¸è¶…è¿‡ç›®æ ‡æ•°é‡
            
        except Exception as e:
            logging.error(f"ä» {source_type} æ”¶é›†å¸–å­æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return collected_urls

    async def fetch_post_json(self, post_url, source_type):
        """è·å–å•ä¸ªå¸–å­çš„JSONæ•°æ®"""
        try:
            # æ„é€ JSON API URL
            base_url = post_url.split('?')[0]
            if base_url.endswith('/'):
                json_url = base_url[:-1] + ".json"
            else:
                json_url = base_url + ".json"
            
            logging.debug(f"è¯·æ±‚API: {json_url}")
            
            # è®¿é—®JSON API
            await self.page.goto(json_url, wait_until='domcontentloaded', timeout=15000)
            
            # ç­‰å¾…éšæœºæ—¶é—´
            api_delay = random.randint(self.delays['api_min'], self.delays['api_max'])
            await self.page.wait_for_timeout(api_delay)
            
            # è·å–JSONå†…å®¹
            pre_element = await self.page.query_selector("pre")
            if not pre_element:
                logging.warning("æœªæ‰¾åˆ°JSONå†…å®¹")
                return None
                
            json_content = await pre_element.text_content()
            if not json_content:
                logging.warning("JSONå†…å®¹ä¸ºç©º")
                return None
            
            # è§£æJSON
            raw_data = json.loads(json_content)
            
            # æå–å¸–å­ä¿¡æ¯
            post_info_raw = raw_data[0]['data']['children'][0]['data']
            comments_tree_raw = raw_data[1]['data']['children']
            
            # è·å–å¸–å­æ—¶é—´æˆ³
            post_utc = post_info_raw.get("created_utc", 0)

            post_data = {
                "title": post_info_raw.get("title", "N/A"),
                "url": post_info_raw.get("url", post_url),
                "body": post_info_raw.get("selftext", ""),
                "upvotes": post_info_raw.get("score", 0),
                "created_utc": post_utc,
                "created_time": self.convert_time(post_utc),
                "total_comments_count": post_info_raw.get("num_comments", 0),
                "source_type": source_type,  # æ–°å¢ï¼šæ ‡è®°æ¥æºç±»å‹
                "post_id": self.extract_post_id(post_url),  # æ–°å¢ï¼šå¸–å­ID
                "comments": []
            }
            
            logging.info(f"è§£æå¸–å­[{source_type}]: {post_data['title'][:50]}...")
            logging.info(f"å‘å¸ƒæ—¶é—´: {post_data['created_time']}")
            
            # è§£æè¯„è®º
            for comment_node in comments_tree_raw:
                parsed_node = self.parse_json_comment(comment_node)
                if parsed_node:
                    post_data["comments"].append(parsed_node)
            
            logging.info(f"è§£æå®Œæˆï¼ŒåŒ…å« {len(post_data['comments'])} æ¡ä¸€çº§è¯„è®º")
            return post_data
            
        except json.JSONDecodeError as e:
            logging.error(f"JSONè§£æå¤±è´¥: {e}")
            return None
        except Exception as e:
            logging.error(f"è·å–å¸–å­æ•°æ®æ—¶å‡ºé”™: {e}")
            return None

    async def crawl_posts(self):
        """ä¸»è¦çš„çˆ¬å–æµç¨‹"""
        completed_normally = False
        current_post_index = 1
        consecutive_failures = 0
        collected_urls_with_source = []
        
        try:
            await self.init_browser()
            
            # åŠ è½½è¿›åº¦
            current_post_index, existing_urls_with_source = self.load_progress()
            
            if existing_urls_with_source:
                collected_urls_with_source = existing_urls_with_source
                logging.info(f"ä»è¿›åº¦æ–‡ä»¶æ¢å¤ï¼Œå·²æœ‰ {len(collected_urls_with_source)} ä¸ªURL")
            else:
                # æ”¶é›†å¸–å­é“¾æ¥
                collected_urls_with_source = await self.collect_post_urls(self.subreddit_url)
            
            if not collected_urls_with_source:
                logging.error("æ²¡æœ‰æ”¶é›†åˆ°ä»»ä½•å¸–å­é“¾æ¥")
                return
            
            # å¼€å§‹çˆ¬å–å¸–å­æ•°æ®
            total_posts = len(collected_urls_with_source)
            logging.info(f"å¼€å§‹çˆ¬å– {total_posts} ä¸ªå¸–å­ï¼Œä»ç¬¬ {current_post_index} ä¸ªå¼€å§‹")
            
            for index in range(current_post_index - 1, total_posts):
                url_item = collected_urls_with_source[index]
                url = url_item["url"]
                source_type = url_item["source"]
                current_post_index = index + 1
                
                logging.info(f"\n[{current_post_index}/{total_posts}] æ­£åœ¨å¤„ç†[{source_type}]: {url}")
                
                # ä¿å­˜è¿›åº¦
                self.save_progress(current_post_index, collected_urls_with_source)
                
                try:
                    post_data = await self.fetch_post_json(url, source_type)
                    
                    if post_data:
                        self.all_posts_data.append(post_data)
                        consecutive_failures = 0  # é‡ç½®å¤±è´¥è®¡æ•°
                        logging.info("âœ… å¸–å­å¤„ç†æˆåŠŸ")
                        
                        # å®šæœŸä¿å­˜æ•°æ®
                        if len(self.all_posts_data) % 10 == 0:
                            self.save_data()
                            
                    else:
                        consecutive_failures += 1
                        logging.warning(f"âŒ å¸–å­å¤„ç†å¤±è´¥ï¼Œè¿ç»­å¤±è´¥æ¬¡æ•°: {consecutive_failures}")
                        
                        if consecutive_failures >= self.max_failures:
                            logging.error("è¿ç»­å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œåœæ­¢çˆ¬å–")
                            break
                    
                    # éšæœºå»¶è¿Ÿ
                    delay = random.randint(self.delays['page_min'], self.delays['page_max'])
                    logging.info(f"ç­‰å¾… {delay/1000:.1f} ç§’åç»§ç»­...")
                    await self.page.wait_for_timeout(delay)
                    
                except Exception as e:
                    consecutive_failures += 1
                    logging.error(f"å¤„ç†å¸–å­æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    traceback.print_exc()
                    
                    if consecutive_failures >= self.max_failures:
                        logging.error("è¿ç»­å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œåœæ­¢çˆ¬å–")
                        break
                    
                    # é”™è¯¯åç­‰å¾…æ›´é•¿æ—¶é—´
                    await self.page.wait_for_timeout(random.randint(5000, 10000))
            
            # å¦‚æœå®Œæ•´å¤„ç†äº†æ‰€æœ‰å¸–å­ï¼Œæ ‡è®°ä¸ºæ­£å¸¸å®Œæˆ
            if current_post_index >= total_posts:
                completed_normally = True
                logging.info("ğŸ‰ æ‰€æœ‰å¸–å­å¤„ç†å®Œæˆ")
            
        except KeyboardInterrupt:
            logging.info("ç”¨æˆ·ä¸­æ–­çˆ¬å–ï¼Œè¿›åº¦å·²ä¿å­˜")
            self.save_progress(current_post_index, collected_urls_with_source)
        except Exception as e:
            logging.error(f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            traceback.print_exc()
            self.save_progress(current_post_index, collected_urls_with_source)
        finally:
            # ä¿å­˜æœ€ç»ˆæ•°æ®
            if self.all_posts_data:
                self.save_data()
            
            # åªæœ‰åœ¨æ­£å¸¸å®Œæˆæ—¶æ‰æ¸…ç†çŠ¶æ€æ–‡ä»¶
            if completed_normally:
                try:
                    if os.path.exists(self.state_file):
                        os.remove(self.state_file)
                    logging.info("çˆ¬å–ä»»åŠ¡å®Œæˆï¼Œå·²æ¸…ç†çŠ¶æ€æ–‡ä»¶")
                except:
                    pass
            else:
                logging.info("ä¿ç•™çŠ¶æ€æ–‡ä»¶ä»¥ä¾¿ä¸‹æ¬¡ç»§ç»­çˆ¬å–")
            
            await self.cleanup()

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self.page:
                await self.page.close()
            if self.browser:
                await self.browser.close()
            if hasattr(self, 'playwright'):
                await self.playwright.stop()
            logging.info("èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            logging.warning(f"æ¸…ç†èµ„æºæ—¶å‘ç”Ÿé”™è¯¯: {e}")

async def main():
    """ä¸»å‡½æ•°"""
    setup_logger()
    
    # ================= é…ç½®åŒºåŸŸ =================
    # æ›¿æ¢ä¸ºä½ æƒ³çˆ¬å–çš„Redditå­ç‰ˆå—URLæˆ–å…·ä½“å¸–å­URL
    target_url = "https://www.reddit.com/r/dogs/"  # ç¤ºä¾‹ï¼šdogså­ç‰ˆå—
    
    # è¾“å‡ºæ–‡ä»¶åï¼ˆå°†è‡ªåŠ¨ä¿å­˜åˆ° ./data/dogs/ ç›®å½•ä¸‹ï¼‰
    output_file = "reddit_data_with_time.json"
    
    # æœ€å¤§çˆ¬å–å¸–å­æ•°é‡
    max_posts = 100
    
    # æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼ï¼ˆå»ºè®®è°ƒè¯•æ—¶è®¾ä¸ºFalseï¼‰
    headless = False
    
    # è‡ªå®šä¹‰é‡‡æ ·æ¯”ä¾‹ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸º65%æ–°/25%çƒ­é—¨/10%æœ€ä½³ï¼‰
    custom_sampling_ratios = {
        'new': 0.6,       # 60% æœ€æ–°å¸–å­
        'top_year': 0.3,  # 30% å¹´åº¦çƒ­é—¨
        'best': 0.1       # 10% æœ€ä½³å¸–å­
    }
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = RedditCrawler(
        subreddit_url=target_url,
        output_file=output_file,
        max_posts=max_posts,
        headless=headless,
        max_failures=3,
        sampling_ratios=custom_sampling_ratios,  # ä¼ å…¥è‡ªå®šä¹‰é‡‡æ ·æ¯”ä¾‹
        delays={
            'page_min': 2000, 'page_max': 5000,
            'action_min': 500, 'action_max': 1500,
            'scroll_min': 3000, 'scroll_max': 8000,  # RedditåŠ è½½æ¯”è¾ƒæ…¢ï¼Œå¢åŠ æ»šåŠ¨å»¶è¿Ÿ
            'api_min': 1000, 'api_max': 2000
        }
    )
    
    # å¼€å§‹çˆ¬å–
    await crawler.crawl_posts()
    
    logging.info("ğŸ‰ Redditçˆ¬å–å®Œæˆï¼")

if __name__ == "__main__":
    asyncio.run(main())